state:int	title:string	subtitle:string	opacity:float	prevenabled:int	nextenabled:int	resetenabled:int
0			0	0	1	0
1	Motivation	Suppose we have a single server and a single client who wishes to send a request to the server. Click on the client to send its request (highlighted circles are clickable).	1	1	1	1
2	Motivation	What happens if the server crashes or becomes unresponsive? This service has a single point of failure; when the server fails, the service can no longer process client requests. How can we make our service withstand server failures?	1	1	1	0
3	Motivation	To add a form of fault-tolerance, we replicate the service state across multiple servers. The client sends a request to all the replica servers, who in turn all update their state. Click on the client to broadcast its request. 	1	1	1	1
4	Motivation	Even if one server crashes, its state is replicated across several other servers. As long as one replica stays alive, the client request can be processed.	1	1	1	1
5	Motivation	What happens if there are multiple clients, all simulatenously sending requests to the servers? Since real-world messages have random latencies, requests could arrive out of order; one replica could get Request 1 first, while another gets Request 2, even if Request 1 was sent before Request 2! The replicas must all process the same client request, or they would become desynchronized. Try to desynchronize the replicas by clicking on the two clients in rapid succession (it may take a few tries)!	1	1	1	1
6	Motivation	We could have a single primary server that chooses which client request to process and broadcasts it to the replicas. Click on either client to send its request. Once the primary has chosen a request, click on it to broadcast it to the replicas.	1	1	1	1
7	Motivation	Again, what happens if the primary server crashes? Just like before, our service has a single point of failure; one machine failing means we can no longer serve clients.	1	1	1	0
8	Motivation	To remove any single point of failure and still have fault-tolerance, we can make the replicas decide amongst themselves which client request to process. This problem is called distributed consensus, since the replicas make a collective decision without a centralized coordinator.	1	1	1	0
9	Paxos	Paxos is the typical algorithm for performing distributed consensus. In the algorithm, servers vote on client requests, and process one request once a majority has voted on it.	1	1	1	0
10	Paxos Clusters	In the Paxos algorithm, a server can take on one of three roles in the consensus process.	1	1	1	0
11	Paxos Clusters	The first role is the <span class='proposer'>proposer</span>, who receives client messages and attempts to get them voted on by proposing them.	1	1	1	0
12	Paxos Clusters	The second role is the <span class='acceptor'>acceptor</span>, who receives proposals for client requests and votes on them.	1	1	1	0
13	Paxos Clusters	The third role is the <span class='learner'>learner</span>, who learns when a proposed request has been elected by the cluster. <span class='learner'>Learners</span> process elected requests, acting as replicas for the server state.	1	1	1	0
14	Paxos Walkthrough	The algorithm starts with the client sending its request to all <span class='proposer'>proposers</span>. <span class='proposer'>Proposers</span> store the request they're advocating for (null initially). <span class='proposer'>Proposers</span> enumerate their request using a serial number unique among the <span class='proposer'>proposers</span> (1 initially). <span class='proposer'>Proposers</span> record how many <span class='acceptor'>acceptors</span> have committed to their serial number, any how many have committed to their request (both 0/1 initially).	1	1	1	1
15	Paxos Prepare Phase	<span class='proposer'>Proposers</span> ask all <span class='acceptor'>acceptors</span> to "prepare" their client request by committing to the request's serial number. <span class='acceptor'>Acceptors</span> store the highest serial number they've seen (-1 initially), and refuse to commit to any lower serial numbers. <span class='acceptor'>Acceptors</span> store the client request associated with the highest serial number they've seen (null initially).	1	1	1	1
16	Paxos Prepare Phase	If <span class='acceptor'>acceptors</span> see a higher serial number than they've seen before, they commit to it, "preparing" for a corresponding client request from a <span class='proposer'>proposer</span>. Crucially, if an acceptor previously voted on a request (not null), they include it in their response, and the proposer must advocate for it in favor of their previous client request. This ensures that once a request is elected by a majority of acceptors, a new one won't replace it. <span class='proposer'>Proposers</span> record how many <span class='acceptor'>acceptors</span> have "prepared" for their serial number (0/1 initially). <span class='proposer'>Proposers</span> stop accepting "prepare" responses once a majority of <span class='acceptor'>acceptors</span> have committed to their serial number (1 in this case).	1	1	1	1
17	Paxos Vote Phase	Once <span class='proposer'>proposers</span> have a majority of "prepare" responses, they ask all <span class='acceptor'>acceptors</span> to vote for their client request. <span class='acceptor'>Acceptors</span> always vote for requests with a serial number at least as high as they highest they've seen. <span class='acceptor'>Acceptors</span> record the client command they voted for (and increase their serial number if they see a higher one at this stage).	1	1	1	1
18	Paxos Vote Phase	If <span class='acceptor'>acceptors</span> see a serial number at least as high as the highest they know about, they vote for the corresponding client request. <span class='proposer'>Proposers</span> record how many <span class='acceptor'>acceptors</span> have voted for their client request (0/1 initially). <span class='proposer'>Proposers</span> stop accepting votes once a majority of <span class='acceptor'>acceptors</span> have voted for their client command (1 in this case).	1	1	1	1
19	Paxos Learn Phase	Once <span class='proposer'>proposers</span> have a majority of votes for their client request, they notify all <span class='learner'>learners</span> about the request. <span class='learner'>Learners</span> store and execute the elected client request (null initially). Paxos ensures that only one client request will ever be voted on by a majority, since any two majorities overlap; If a majority of <span class='acceptor'>acceptors</span> has voted on a request, any <span class='proposer'>proposer</span> who receives a majority of <span class='acceptor'>acceptor</span> "prepare" responses will see the request and start advocating for it in favor of their current client request.	1	1	1	1
20	Paxos Learn Phase	Once <span class='learner'>learners</span> learn of a client request that a majority of <span class='acceptor'>acceptors</span> has voted on, they are free to service the request and respond to the client, if necessary. Paxos ensures that the elected request will not change, so all <span class='learner'>learners</span> will eventually learn of the same client request and remain in sync. The <span class='learner'>learners</span> are the replicas of a Paxos cluster, so the election process yields a replicated service that can tolerate a significant number of faults. Since Paxos only needs a majority of <span class='acceptor'>acceptors</span> for progress to be made, a Paxos cluster can tolerate up to a minority (half) of <span class='acceptor'>acceptors</span> failing.	1	1	1	1
21	Step-By-Step Demo (Small)	Now, let's run through the algorithm, start to finish. Click through the Paxos instance until the client gets a response, and try to name the phases of the consensus process as they happen. If there's a state you can't explain, you can go back to the walkthrough and re-familiarize yourself. If you need to reset the algorithm, please wait until messages stop moving, as the visualization isn't quite perfect T_T (otherwise you may have to reset twice).	1	1	1	1
22	Step-By-Step Demo (Large)	Until now, we've been using one instance of each role in the consensus process. Of course, Paxos is designed to tolerate more than one fault, so typical clusters are set up with 3, 5, or sometimes 7 instances of each role. Click through the Paxos algorithm with 3 instances of each role and try to recognize the same phases of consensus as before, now with more instances of each role. Note that since we have 3 <span class='acceptor'>acceptors</span>, <span class='proposer'>proposers</span> now need 2 distinct "prepare" replies and 2 distinct votes from <span class='acceptor'>acceptors</span> (up from 1) to elect a client request. Note that even though there is a circle drawn for each instance of a role, a typical real-world Paxos cluster would use one machine as a <span class='proposer'>proposer</span>, an <span class='acceptor'>acceptor</span>, and a <span class='replica'>replica</span>; the cluster above would likely be composed of 3 physical machines (not including the client).	1	1	1	1
23	Automated Demo (Large)	Now that you're (hopefully) familiar with Paxos at a step-by-step level, let's increase the workload for the service by adding a second client. Recall that the objective for Paxos is to elect a single client request, so the algorithm will ultimately choose either Request1 or Request2. In order to process the two commands in some sequence, the service must restart the Paxos instance after the first command is chosen, and decide on a second command for all the learners (replicas) to serve (typically, both clients would be retrying their commands until success). In a real-world deployment, parallel instances of Paxos would be spawned to simultaneously elect several client requests. Start a run by clicking on either (or both) clients. During the run, click on any <span class='proposer'>proposer</span>, <span class='acceptor'>acceptor</span>, or <span class='replica'>replica</span> to kill it; recall that at least 2 <span class='acceptor'>acceptors</span> must live in a cluster of 3 to elect commands (try killing 2+ <span class='acceptor'>acceptors</span> before any <span class='proposer'>proposer</span> has collected 2 votes, and see what happens!)	1	1	0	1